%\documentclass[12pt, preprint]{aastex}
\documentclass[twocolumn,times]{aastex62}
%\documentclass[preprint,times]{aastex6}

% these lines seem necessary for pdflatex to get the paper size right
\pdfpagewidth 8.5in
\pdfpageheight 11.0in


\usepackage{epsf,color,amsmath}

\usepackage{cancel}

\newcommand{\sfrac}[2]{\mathchoice%
  {\kern0em\raise.5ex\hbox{\the\scriptfont0 #1}\kern-.15em/
    \kern-.15em\lower.25ex\hbox{\the\scriptfont0 #2}}
  {\kern0em\raise.5ex\hbox{\the\scriptfont0 #1}\kern-.15em/
    \kern-.15em\lower.25ex\hbox{\the\scriptfont0 #2}}
  {\kern0em\raise.5ex\hbox{\the\scriptscriptfont0 #1}\kern-.2em/
    \kern-.15em\lower.25ex\hbox{\the\scriptscriptfont0 #2}} {#1\!/#2}}


\newcommand{\castro}{{\sf Castro}}
\newcommand{\maestro}{{\sf Maestro}}

\newcommand{\nablab}{{\mathbf{\nabla}}}
\newcommand{\Ub}{\mathbf{U}}
\newcommand{\gb}{\mathbf{g}}
\newcommand{\omegadot}{\dot{\omega}}
\newcommand{\Sdot}{\dot{S}}
\newcommand{\ddx}[1]{{\frac{{\partial#1}}{\partial x}}}
\newcommand{\ddt}[1]{{\frac{{\partial#1}}{\partial t}}}
\newcommand{\odt}[1]{{\frac{{d#1}}{dt}}}
\newcommand{\divg}[1]{{\nablab \cdot \left (#1\right)}}

\newcommand{\Ic}{\mathcal{I}}
\newcommand{\smax}{{s_\mathrm{max}}}

\usepackage{bm}

\newcommand{\Uc}{{\bm{\mathcal{U}}}}
\newcommand{\Fb}{\mathbf{F}}
\newcommand{\Sc}{\mathbf{S}}

\newcommand{\xv}{{(x)}}
\newcommand{\yv}{{(y)}}
\newcommand{\zv}{{(z)}}

\newcommand{\ex}{{\bf e}_x}
\newcommand{\ey}{{\bf e}_y}
\newcommand{\ez}{{\bf e}_z}

\newcommand{\Ab}{{\bf A}}
\newcommand{\Sq}{{\bf S}_\qb}
\newcommand{\Sqhydro}{{\Sq^{\mathrm{hydro}}}}
\newcommand{\qb}{{\bf q}}

\newcommand{\Shydro}{{{\bf S}^{\mathrm{hydro}}}}
\newcommand{\Rb}{{\bf R}}
\newcommand{\Rq}{{\bf R}}
\newcommand{\Adv}[1]{{\left [\mathcal{A} \left(#1\right)\right]}}
\newcommand{\Advt}[1]{{\left [\mathcal{\tilde{A}} \left(#1\right)\right]}}
\newcommand{\Advs}[1]{{\mathcal{A} \left(#1\right)}}

\setlength{\marginparwidth}{0.75in}
\newcommand{\MarginPar}[1]{\marginpar{\vskip-\baselineskip\raggedright\tiny\sffamily\hrule\smallskip{\color{red}#1}\par\smallskip\hrule}}

\begin{document}
%======================================================================
% Title
%======================================================================
\title{CASTRO: A New Compressible Astrophysical Solver. IV. Performance Portability}

\shorttitle{}
\shortauthors{}


\correspondingauthor{Michael Zingale}

\collaboration{the Castro Development Team}

\author[0000-0003-2103-312X]{Ann S. Almgren}
\affil{Center for Computational Sciences and Engineering, Lawrence Berkeley National Laboratory}

\author[0000-0002-3185-9809]{Maria Barrios Sazo}
\affil{Department of Physics and Astronomy, Stony Brook University}

\author{Kevin Gott}
\affil{National Energy Research Scientific Computing Center}

\author[0000-0002-1530-781X]{Alice Harpole}
\affil{Department of Physics and Astronomy, Stony Brook University}

\author[0000-0003-0439-4556]{Max P.~Katz}
\affil{NVIDIA Corporation}

\author[0000-0003-2300-5165]{Donald Willcox}
\affil{Center for Computational Sciences and Engineering, Lawrence Berkeley National Laboratory}

\author[0000-0001-8092-1974]{Weiqun Zhang}
\affil{Center for Computational Sciences and Engineering, Lawrence Berkeley National Laboratory}

\author[0000-0001-8401-030X]{Michael Zingale}
\affil{Department of Physics and Astronomy, Stony Brook University}
\email{michael.zingale@stonybrook.edu}




%======================================================================
% Abstract and Keywords
%======================================================================
\begin{abstract}
We describe a performance portable version of the \castro\ hydrodynamics
code that is able to achieve excellent scaling on both CPU and GPU-based
supercomputers.
\end{abstract}

\keywords{hydrodynamics---methods: numerical}

%======================================================================
% Introduction
%======================================================================
\section{Introduction}\label{Sec:Introduction}

Supercomputer architectures have evolved a lot over the past
decades~\citep{bell:2015}.  Shared memory vector machines with custom
processors dominated early, displaced in the early 1990s by
distributed memory, massively parallel architectures built upon
commodity CPUs where the message passing interface (MPI) became the
dominant library for achieving parallelism.  As core counts increased,
threading because increasingly important, and OpenMP became the
standard for threading in high-performance computing, with hybrid
MPI+OpenMP approaches to parallelism.  In the race to increase
performance while keeping electrical power consumption reasonable,
accelerators were increasingly turned to, include GPUs and many-core
chips like the Intel Phi series.  The simulation codes we use in
astrophysics need to keep pace with these developments, or they will
not be able to run on the next generation of supercomputers.  In
particular, GPUs are dominant now, so existing algorithms need to be
ported to run on GPUs, or new codes need to be written from scratch
targeting the new architecture.

There are a number of descriptions of GPU-enabled astrophysical hydro
codes, including
\citet{gamer,cholla,fargo3d,pekkila:2017,caplan:2018,racz:2018,liska:2018,goz:2018,wang:2010,Padioleau2019,Bryan2014,Kulikov2014,Bedorf2012,Schive2018,Potter2016} \MarginPar{todo:
  group by AMR/no-AMR, GPU exclusive, Kokkos \citep{CarterEdwards2014}, ...}

GPU-enabled integration has been explored by \citep{brock:2015}.


In this paper, we describe our approach to performance portability
with the \castro\ multiphysics simulation code..  Our goal is to have
the same computational kernels run efficiently on CPUs (including
manycore processors) and GPUs.  We describe the work we needed to do
to enable this portability and show performance metrics for a reacting
flow problem in astrophysics.


\subsection{The Castro hydrodynamics code}

\castro\ is a compressible (radiation-)hydrodynamics simulation code built on
the AMReX adaptive mesh refinement framework \citep{castro,castro2,castro3}.  It
includes full self-gravity using multigrid methods with isolated boundaries
\citep{katz:2016}, works with general equations of state \citep{zingalekatz}
and arbitrary nuclear reaction networks.  Castro has been used for science investigations of
...

The original computing architectures targeted were

\castro\ development began with \cite{castro}, with the goal of
creating an astrophysical hydrodynamics code using a modern adaptive
mesh refinement library (BoxLib at the time) and unsplit hydrodynamics
techniques.  The initial application area was nuclear astrophysics,
including core-collapse supernovae, so it was designed to work with a
general equation of state~\cite{zingalekatz} and reaction network, and
with radiation transport (through flux limited diffusion,
\citealt{castro2,castro3}).  \castro\ also acted as the compressible
counterpart to our low Mach number hydrodynamics code \maestro,
sharing much of the same infrastructure, and allowing for simulations
to be restarted from the low Mach regime into the compressible
regime~\cite{malone:2014}.  \castro\ has been applied to various
progenitor models of Type Ia
supernova~\cite{malone:2014,katz:2016,polin:2019}, exoplanet
dynamics~\cite{ryu:2018}, core-collapse and population III
supernovae~\cite{chen:2014,dolence:2015,chen:2017}, and X-ray bursts~\cite{astronum:2018}.



\castro\ is developed using a fully open-development model, with the
code managed in git and hosted on
github\footnote{\url{https://github.com/amrex-astro/Castro/}}.
Contributions from the community are welcomed via github issues and
pull requests.  For the science projects that the core development
team are working on, all problem setup, input files, auxillary data,
and anything else needed for the science is stored in the main github
repo, allowing for the community to contribute to the science and
reproduce results as they are published.

\section{Performance portable compute kernels}


\subsection{Offloading to GPUs}

CUDA Fortran is not as well supported as CUDA C/C++.  In particular,
CUDA Fortran does not automatically create both the host and device
kernel, necessitating one to do this themselves.  Usually this means
having two copies of a source routine, one marked up with device
attributes.  Keeping them in sync essentially requires that this
action be scripted, but in our experience, this scripting is fragile
because of the need to deal with Fortran syntax, preprocessor
directives, ...  Thus the pure CUDA route for the compute kernels makes the most sense
with C/C++ kernels, and not Fortran kernels.

OpenACC


OpenMP

Discuss frameworks vs.\ doing it outselves




\subsection{Example of advancing a grid}

Describe mfiter structure

Describe MPI, MPI+OpenMP w/ tiling, MPI + CUDA

\begin{figure*}
\centering
\includegraphics[width=0.3\textwidth]{gpu_1}
\includegraphics[width=0.3\textwidth]{gpu_2}
\includegraphics[width=0.3\textwidth]{gpu_3}
\caption{\label{fig:loops} caption}
\end{figure*}

\subsection{Summary of changes required}

The main hydrodynamics solver in \castro\ is an unsplit corner
transport upwind \citep{ppmunsplit} piecewise parabolic method
\citep{ppm} scheme.  This uses full corner coupling, requiring a
number of transverse Riemann problem solutions.


Boundary conditions require special attention---threadblock must include
all zones that participate in the boundary.

\section{Reaction networks}

\section{Scaling}

Our current focus is on offloading the hydrodynamics and reactions to
GPUs, so we will achieve the best GPU performance for problems without
self-gravity.

\subsection{Pure hydrodynamics}

\subsection{Reaction test}

\subsection{X-ray burst}

This problem uses constant gravity, so all of the physics can be done on the GPUs.


\section{Summary}

Future work: gravity on GPUs (multigrid or multipole), port of \maestro\ following
the same ideas.

\software{MPICH, GCC, Castro, AMReX, python, matplotlib}

\facilities{OLCF, NERSC}

\acknowledgements \castro\ is freely available at
\url{http://github.com/AMReX-Astro/Castro}.  The work at Stony Brook
was supported by DOE/Office of Nuclear Physics grant DE-FG02-87ER40317
and NSF award AST-1211563.  An award of computer time was provided by
the Innovative and Novel Computational Impact on Theory and Experiment
(INCITE) program.  This research used resources of the Oak Ridge
Leadership Computing Facility at the Oak Ridge National Laboratory,
which is supported by the Office of Science of the U.S. Department of
Energy under Contract No. DE-AC05-00OR22725.  We thank NVIDIA Corporation
for the donation of a Titan X Pascal and Titan V used in this research.





%======================================================================
% References
%======================================================================

\bibliographystyle{apj}
\bibliography{ws}

\end{document}
